# -*- coding: utf-8 -*-
"""stroke.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dmUaV7Exb-CpAJ1f5q7A1zqod3Ek5hHY

# Q2

## getting data from url
"""

import pandas as pd

file_id = '1w_4xTCDYcY-lu3p1zB3pT2EjM7CuXIaa'
url = f'https://drive.google.com/uc?export=download&id={file_id}'
df = pd.read_csv(url)

print(df.T)

"""## part 1

<div style="text-align: right;", dir="rtl">
در بسیاری از مسائل دسته‌بندی، تعداد نمونه‌های مختلف برای هر کلاس برابر نیست. برای حل این مشکل، سه راهکار متداول وجود دارد:

 **نمونه‌برداری متعادل (Resampling)**
   - **افزایش نمونه‌ها (Oversampling):** در این روش، با تکرار نمونه‌های کلاس‌های کم‌تعداد، تعداد نمونه‌ها را متعادل می‌کنیم. به این ترتیب، مدل به تعداد بیشتری از نمونه‌های کلاس‌های کم‌تعداد دسترسی پیدا می‌کند. یک روش مشهور در این زمینه، تکنیک **SMOTE** است که با ایجاد نمونه‌های مصنوعی جدید از داده‌های کم‌تعداد، تعداد آنها را افزایش می‌دهد.
   - **کاهش نمونه‌ها (Undersampling):** در این روش، با حذف تعدادی از نمونه‌های کلاس‌های پر‌تعداد، تعداد نمونه‌ها را کاهش می‌دهیم. این روش باعث می‌شود مدل برای هر کلاس، تعداد متعادل‌تری از نمونه‌ها را ببیند، اما خطر از دست دادن اطلاعات مهم نیز وجود دارد.

 **استفاده از وزن‌دهی (Class Weighing)**
   - در این روش، وزن هر کلاس در طول آموزش مدل به‌صورت دینامیک تنظیم می‌شود. به کلاس‌های کم‌تعداد وزن بیشتری اختصاص داده می‌شود تا مدل، آن‌ها را در فرایند آموزش به‌اندازه کافی مهم بداند. این روش برای الگوریتم‌های مختلف مثل درخت تصمیم، رگرسیون لجستیک و شبکه‌های عصبی قابل استفاده است و بدون نیاز به تغییر داده‌ها، تعادل نسبی را حفظ می‌کند.

 **استفاده از الگوریتم‌های مخصوص داده‌های نامتوازن**
   - برخی الگوریتم‌ها مانند **جنگل تصادفی متوازن (Balanced Random Forest)** و **طبقه‌بندهای تطبیقی (Adaptive Boosting)** برای مقابله با داده‌های نامتوازن طراحی شده‌اند. این الگوریتم‌ها می‌توانند در زمان یادگیری، به داده‌های نامتوازن اهمیت بیشتری بدهند و عملکرد بهتری در دسته‌بندی کلاس‌های کم‌تعداد داشته باشند.

   روش سوم برای مقابله با داده‌های نامتوازن، استفاده از **الگوریتم‌های مخصوص داده‌های نامتوازن** است. در این روش‌ها، از مدل‌ها یا تکنیک‌هایی استفاده می‌شود که به‌طور خاص برای مدیریت داده‌هایی با تعداد نمونه‌های نامساوی طراحی شده‌اند. در ادامه، دو الگوریتم معروف که به این منظور استفاده می‌شوند را معرفی می‌کنیم:

 **جنگل تصادفی متوازن (Balanced Random Forest)**
   - **توضیح:** جنگل تصادفی متوازن یک نوع خاص از جنگل تصادفی است که برای بهبود عملکرد روی داده‌های نامتوازن طراحی شده است. در هر مرحله ساخت درخت، این الگوریتم به‌جای انتخاب تصادفی داده‌ها از کل مجموعه، نمونه‌ها را به‌گونه‌ای انتخاب می‌کند که تعداد نمونه‌های هر کلاس برابر باشد. به این صورت، هر درخت در جنگل تصادفی به یک مجموعه متعادل از نمونه‌ها آموزش داده می‌شود.
   - **مزایا:** این روش از مزایای روش‌های نمونه‌برداری و ترکیب آنها با قدرت یادگیری جنگل تصادفی بهره می‌برد و به دسته‌بندی دقیق‌تر کلاس‌های کم‌تعداد کمک می‌کند. جنگل تصادفی متوازن بدون نیاز به تغییر کل داده‌ها، از طریق تکرار نمونه‌برداری متعادل، به مشکل نامتوازنی پاسخ می‌دهد.
 **تقویت تطبیقی (Adaptive Boosting یا AdaBoost)**
   - **توضیح:** تقویت تطبیقی یک تکنیک تقویت‌کننده (Boosting) است که با تمرکز بر نمونه‌هایی که به‌درستی دسته‌بندی نشده‌اند، مدل‌های خود را بهبود می‌دهد. برای داده‌های نامتوازن، AdaBoost به نمونه‌های کلاس‌های کم‌تعداد وزن بیشتری اختصاص می‌دهد و آنها را در مراحل بعدی آموزش برجسته‌تر می‌کند. به این ترتیب، مدل توجه بیشتری به نمونه‌های کم‌تعداد دارد و برای آنها تلاش بیشتری می‌کند.
   - **مزایا:** AdaBoost می‌تواند مدل‌هایی بسازد که در شناسایی کلاس‌های کم‌تعداد به‌مراتب بهتر عمل کنند و نرخ خطای کمتری روی این کلاس‌ها داشته باشند. از آنجا که هر مدل پایه در AdaBoost، تلاش می‌کند تا خطای مدل‌های قبلی را اصلاح کند، این روش به‌طور طبیعی برای داده‌های نامتوازن بسیار مناسب است.

این الگوریتم‌ها، با تمرکز بر روی نمونه‌های کم‌تعداد و تنظیم ساختار یادگیری، به مدل کمک می‌کنند که بدون نیاز به تغییر گسترده در داده‌ها، کلاس‌های نادر را با دقت بالاتری تشخیص دهد.
</div>

## part 2

### Data analyzing

Display basic structure
"""

df.shape

df.columns

"""Provides an overview of column types and null values"""

df.info()

"""BMI and smoking_status is null in some cases

Summary statistics for numerical data and summary for categorical columns
"""

df.describe()

"""Summary Statistics
Age:

The mean age is 42.2 years, with a range from 0.08 to 82 years.
Median age is 44, suggesting a generally mature population, though there are outliers at the younger end.


Hypertension:

9.4% of the individuals have hypertension (mean = 0.093).
Majority (90.6%) do not have hypertension.


Heart Disease:

4.8% of the individuals have heart disease (mean = 0.048).
The dataset shows that heart disease is less common than hypertension.


Average Glucose Level:

The mean glucose level is 104.5 mg/dL, with a significant standard deviation (43.1).
The range (55.0 to 291.05 mg/dL) suggests a few individuals may have very high glucose levels, indicating potential cases of diabetes or pre-diabetes.


BMI:

The mean BMI is 28.6, which is slightly above the normal range (18.5–24.9), indicating that many individuals in this dataset may be overweight or obese.
There are some extreme values, as the maximum BMI recorded is 97.6.


Stroke:

Only 1.8% of the individuals have experienced a stroke, indicating that strokes are relatively rare in this sample.
"""

df.describe(include='object')

"""let’s check its distribution to see if there is a class imbalance"""

df['stroke'].value_counts(normalize=True)

"""This confirms that strokes are relatively rare in this population, with only a small proportion (1.8%) of individuals affected. This low prevalence suggests that any analysis focusing on stroke risk factors will need to carefully handle the imbalance between the two classes, possibly using techniques such as resampling or stratified analysis to ensure that results are not skewed by the high proportion of non-stroke cases.

A breakdown of missing values by column helps us understand data quality.
"""

df.isnull().sum()

"""Analyzing Key Columns Individually"""

import matplotlib.pyplot as plt

# Set up the figure and axes
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle('Distributions of Age, Gender, Hypertension, Heart Disease, and Smoking Status')

# Plot age distribution (histogram)
df['age'].plot(kind='hist', bins=20, ax=axes[0, 0], color='skyblue')
axes[0, 0].set_title('Age Distribution')
axes[0, 0].set_xlabel('Age')
axes[0, 0].set_ylabel('Frequency')

# Plot gender distribution (bar plot)
df['gender'].value_counts(normalize=True).plot(kind='bar', ax=axes[0, 1], color='salmon')
axes[0, 1].set_title('Gender Distribution')
axes[0, 1].set_xlabel('Gender')
axes[0, 1].set_ylabel('Proportion')

# Plot hypertension distribution (bar plot)
df['hypertension'].value_counts(normalize=True).plot(kind='bar', ax=axes[1, 0], color='lightgreen')
axes[1, 0].set_title('Hypertension Distribution')
axes[1, 0].set_xlabel('Hypertension')
axes[1, 0].set_ylabel('Proportion')

# Plot heart disease distribution (bar plot)
df['heart_disease'].value_counts(normalize=True).plot(kind='bar', ax=axes[1, 1], color='lightcoral')
axes[1, 1].set_title('Heart Disease Distribution')
axes[1, 1].set_xlabel('Heart Disease')
axes[1, 1].set_ylabel('Proportion')

# Adjust layout for readability
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

print(df['gender'].unique())
print(df['ever_married'].unique())
print(df['work_type'].unique())
print(df['Residence_type'].unique())
print(df['smoking_status'].unique())

"""For numerical features, checking the correlation matrix is useful to identify relationships between features (e.g., age, bmi, avg_glucose_level), which might have predictive value for stroke."""

import numpy as np
import pandas as pd

# Replace empty strings with np.nan
df.replace('', np.nan, inplace=True)

# Clean and standardize column values
df['gender'] = df['gender'].str.strip().str.capitalize()
df['ever_married'] = df['ever_married'].str.strip().str.capitalize()
df['work_type'] = df['work_type'].str.strip()
df['Residence_type'] = df['Residence_type'].str.strip()
df['smoking_status'] = df['smoking_status'].str.strip()

# Fill NaN values in 'smoking_status' with a placeholder ('unknown')
df['smoking_status'] = df['smoking_status'].fillna('unknown')

# Map categorical values to numeric values
df['gender'] = df['gender'].map({'Male': 1, 'Female': 0, 'Other': 2})
df['ever_married'] = df['ever_married'].map({'Yes': 1, 'No': 0})
df['work_type'] = df['work_type'].map({'Govt_job': 4, 'Self-employed': 3, 'Never_worked': 2, 'children': 1, 'Private': 0})
df['Residence_type'] = df['Residence_type'].map({'Rural': 1, 'Urban': 0})
df['smoking_status'] = df['smoking_status'].map({'smokes': 2, 'never smoked': 1, 'formerly smoked': 0, 'unknown': -1})

df.corr()

"""General Observations


Age and Health Factors:

Age shows a moderate positive correlation with bmi (0.358897) and avg_glucose_level (0.237627), suggesting that as people age, they tend to have higher BMI and average glucose levels.
Age also has a moderate positive correlation with stroke (0.156049), indicating that age is a risk factor for stroke.


BMI and Avg Glucose Level:

There is a positive correlation between bmi and avg_glucose_level (0.191295), suggesting that higher BMI may be associated with higher glucose levels, a potential indicator of metabolic or cardiovascular issues.


Work Type:

There is a positive but weak correlation between work_type and age (0.181983), suggesting that certain work types might be associated with older or younger populations.


Stroke Correlations:

avg_glucose_level (0.078917) and age (0.156049) both have positive correlations with stroke, although these are relatively weak. This suggests that while these factors might contribute to stroke risk, there may be other factors not represented here that also play a significant role.
smoking_status has a slightly negative correlation with stroke (-0.023068), indicating no clear association in this dataset.


work_type, Residence_type: These features have very low correlations with stroke (close to zero). You may consider removing these if they do not have a stronger relationship with stroke.
"""

df.T

"""### Data Cleaning

Removing Duplicates: Identifying and removing duplicate records in the dataset to ensure that no data point is counted multiple times.
"""

df.drop_duplicates()

"""13293 samples don't have smoking_ status, also its ccorrelation is about 0.01 so we can drop it."""

df.drop('smoking_status', axis=1, inplace=True)
df.T

"""other attributes are not highly corrilated with each other, they don't have much missing values and their corrilation is acceptable we can still consider them.

some values of bmi is extremly high, we replace them with higher and lower band using IQR
"""

import pandas as pd

# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = df['bmi'].quantile(0.25)
Q3 = df['bmi'].quantile(0.75)

# Calculate the IQR
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Replace the values higher than upper_bound with upper_bound
# and values lower than lower_bound with lower_bound
df['bmi'] = df['bmi'].apply(
    lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x)
)

"""replacing Nan with mean in bmi"""

# Calculate the mean of the 'bmi' column, ignoring NaN values
bmi_mean = df['bmi'].mean()

# Fill null values in 'bmi' column with the calculated mean
df['bmi'].fillna(bmi_mean, inplace=True)

"""doing the same with avg_glucose_level"""

import pandas as pd

Q1 = df['avg_glucose_level'].quantile(0.25)
Q3 = df['avg_glucose_level'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df['avg_glucose_level'] = df['avg_glucose_level'].apply(
    lambda x: upper_bound if x > upper_bound else (lower_bound if x < lower_bound else x)
)

"""here we do a normalization on numeric data"""

df['avg_glucose_level'] = (df['avg_glucose_level'] - df['avg_glucose_level'].min()) / (df['avg_glucose_level'].max() - df['avg_glucose_level'].min())
df['age'] = (df['age'] - df['age'].min()) / (df['age'].max() - df['age'].min())
df['bmi'] = (df['bmi'] - df['bmi'].min()) / (df['bmi'].max() - df['bmi'].min())

"""we dealed with categorical data gender and .. before with mapping a value to each class.

we also handeled Inconsistent Data before with capitalizing.

applying one-hot encoding on work type (because it has 5 classes)
"""

df = pd.get_dummies(df, columns=['work_type'])
df.T

df.info()

"""## part 3

SMOTE generates synthetic samples for the minority class rather than simply duplicating existing samples, which can improve model performance in some cases.


SMOTE (Synthetic Minority Over-sampling Technique) is a popular approach for handling imbalanced datasets by generating synthetic samples for the minority class. Here’s how it works:

1. Identify Minority Class Samples
SMOTE first identifies instances of the minority class in the dataset. For each minority instance, it identifies a set of k nearest neighbors within the minority class (by default, k=5).

2. Create Synthetic Samples
For each minority instance, SMOTE randomly selects one or more of its k-nearest neighbors and generates synthetic samples along the line segment connecting the minority instance and its neighbor. This synthetic sample lies somewhere in between the two points, helping to represent new, realistic minority class samples.
"""

import pandas as pd
from imblearn.over_sampling import SMOTE

y = df['stroke']
X = df.drop('stroke', axis=1)

# Initialize SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Combine the resampled data back into a DataFrame
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['stroke'] = y_resampled

# Display the resampled DataFrame
print(df_resampled)

"""train-test split:"""

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

df_resampled = shuffle(df_resampled, random_state=60)

X = df_resampled.drop(columns='stroke')
y = df_resampled['stroke']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

"""converting boolean columns to integers"""

# Convert boolean columns to integers in DataFrames
X_train = X_train.astype({col: 'int' for col in X_train.select_dtypes('bool').columns})
X_test = X_test.astype({col: 'int' for col in X_test.select_dtypes('bool').columns})
# Fill NaN values with the column mean
X_train = np.nan_to_num(X_train, nan=np.nanmean(X_train, axis=0))
X_test = np.nan_to_num(X_test, nan=np.nanmean(X_test, axis=0))

"""#### saving data"""

import pandas as pd
import numpy as np
pd.DataFrame(X_train).to_csv('X_train.csv', index=False)
pd.DataFrame(X_test).to_csv('X_test.csv', index=False)
pd.DataFrame(y_train).to_csv('y_train.csv', index=False)
pd.DataFrame(y_test).to_csv('y_test.csv', index=False)

print("Data saved to CSV files.")

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

import pandas as pd
X_train = pd.read_csv('X_train.csv').values
X_test = pd.read_csv('X_test.csv').values
y_train = pd.read_csv('y_train.csv').values.flatten()
y_test = pd.read_csv('y_test.csv').values.flatten()

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""## part 4

implementing KNN To implement a custom K-Nearest Neighbors (KNN) classifier, we’ll calculate the distances manually, predict based on the majority class of the closest neighbors, and then use sklearn.metrics for evaluation.

Explanation of the Code

Initialization: We define a KNNClassifier class with a k parameter for the number of neighbors.

Training: The fit method stores the training data (X_train and y_train).
Prediction:

For each test example, we compute the Euclidean distance to each training example.

We find the indices of the k nearest neighbors.
We extract the labels of these neighbors and predict the most common label among them.

Evaluation: We use classification_report from sklearn.metrics to evaluate the model's performance.

This code will give you a classification report with precision, recall, and F1-score for each class in y_test.
"""



import cupy as cp
from sklearn.metrics import classification_report
from collections import Counter
import time
import numpy as np
class KNNClassifier:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X_train, y_train):
        # Transfer data to GPU
        self.X_train = cp.asarray(X_train)
        self.y_train = cp.asarray(y_train)

    def predict(self, X_test):
        start_time = time.time()
        total_samples = len(X_test)
        predictions = []

        # Convert test data to GPU array
        X_test = cp.asarray(X_test)

        for i, x in enumerate(X_test):
            # Print progress
            elapsed_time = time.time() - start_time
            progress = (i + 1) / total_samples * 100
            estimated_total_time = elapsed_time / (i + 1) * total_samples
            remaining_time = estimated_total_time - elapsed_time

            print(f"Progress: {progress:.2f}% | Elapsed time: {elapsed_time:.2f}s | Estimated time left: {remaining_time:.2f}s", end="\r")

            predictions.append(self._predict(x))

        print()  # Newline after the progress bar
        return cp.asnumpy(cp.array(predictions))  # Convert back to CPU (NumPy) array

    def _predict(self, x):
        # Compute distances between x and all examples in the training set (using GPU)
        distances = cp.linalg.norm(self.X_train - x, axis=1)

        # Sort by distance and return indices of the first k neighbors
        k_indices = cp.argsort(distances)[:self.k]

        # Extract the labels of the nearest neighbors
        k_nearest_labels = self.y_train[k_indices]

        # Return the most common class label
        most_common = Counter(k_nearest_labels.get())  # Transfer back to CPU for Counter
        return most_common.most_common(1)[0][0]


# Convert DataFrames to NumPy arrays (if not already)
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train).flatten()

knn = KNNClassifier(k=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(classification_report(y_test, y_pred))

"""1. Class 0:
Precision: 0.72, indicating that 72% of the instances labeled as 0 by the model are truly 0.
Recall: 0.59, meaning that the model correctly identified 59% of all actual 0 instances.
F1-Score: 0.64, a balance between precision and recall for this class.
2. Class 1:
Precision: 0.64, so 64% of the instances labeled as 1 by the model are truly 1.
Recall: 0.76, indicating that the model identified 76% of all true 1 instances.
F1-Score: 0.69, balancing precision and recall for this class.
3. Overall Metrics:
Accuracy: 0.67, showing that 67% of all predictions (regardless of class) are correct.
Macro Average: 0.67 across precision, recall, and F1-score, suggesting the model performs similarly across both classes.
Weighted Average: 0.67 across precision, recall, and F1-score, weighted by the number of samples in each class.

### testing the sklearn regresion model
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report


# Initialize Logistic Regression model
log_reg = LogisticRegression(max_iter=200)

# Train the model
log_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = log_reg.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred))

"""as we commpare these 2 models, our result for knn is reasonable and can improve with changing hyper parameters.

## part 5

Standardization and Binning the age

due the diagram in part 2 we can find that age distribution is not normal

Standardization: The StandardScaler scales the age column so that it has a mean of 0 and a standard deviation of 1, storing the result in a new column, age_standardized.

Binning: pd.cut() is used to bin the standardized age values into categories like "Young," "Middle-aged," etc., based on defined bin ranges.
"""

X_train.T

import pandas as pd
X_train = pd.DataFrame(X_train, columns=['id','gender','age','hypertension','heart_disease','ever_married','Residence_type','avg_glucose_level','bmi','work_type_0','work_type_1','work_type_2','work_type_3','work_type_4'])
# Step 1: Calculate the mean and standard deviation of the 'age' column
age_mean = X_train['age'].mean()
age_std = X_train['age'].std()

# Step 2: Standardize the 'age' column manually
X_train['age_standardized'] = (X_train['age'] - age_mean) / age_std

# Step 3: Binning the standardized 'age' column
# Define age bins and labels (these can be customized)
age_bins = [-float('inf'), -1, 0, 1, float('inf')]
age_labels = ['Young', 'Middle-aged', 'Older', 'Senior']

X_train['age_binned'] = pd.cut(X_train['age_standardized'], bins=age_bins, labels=age_labels)

# Display the updated DataFrame
print(X_train)

X_train = X_train.drop(['age', 'age_standardized'], axis=1)
print(X_train)

label_mapping = {'Young': 0, 'Middle-aged': 1, 'Older': 2, 'Senior': 3}
X_train['age_binned'] = X_train['age_binned'].map(label_mapping)

print(X_train)

"""apply binding to avg_glucose_level and bmi"""

import pandas as pd

bmi_bins = [-float('inf'), 18.5, 24.9, 29.9, float('inf')]  # Underweight, Normal, Overweight, Obese
bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese']

# Glucose level bins and labels
glucose_bins = [-float('inf'), 70, 99, 125, float('inf')]  # Low, Normal, Prediabetes, Diabetes
glucose_labels = ['Low', 'Normal', 'Prediabetes', 'Diabetes']

# Step 2: Apply binning
X_train['bmi_binned'] = pd.cut(X_train['bmi'], bins=bmi_bins, labels=bmi_labels)
X_train['glucose_binned'] = pd.cut(X_train['avg_glucose_level'], bins=glucose_bins, labels=glucose_labels)

# Step 3: Map labels to numerical values
bmi_mapping = {'Underweight': 0, 'Normal': 1, 'Overweight': 2, 'Obese': 3}
glucose_mapping = {'Low': 0, 'Normal': 1, 'Prediabetes': 2, 'Diabetes': 3}

X_train['bmi_binned_numeric'] = X_train['bmi_binned'].map(bmi_mapping)
X_train['glucose_binned_numeric'] = X_train['glucose_binned'].map(glucose_mapping)

# Display the updated DataFrame
print(X_train)

X_train = X_train.drop(['bmi_binned', 'glucose_binned','avg_glucose_level','bmi'], axis=1)
print(X_train)

X_train = X_train.rename(columns={'age_binned': 'age'})
X_train = X_train.rename(columns={'bmi_binned_numeric': 'bmi'})
X_train = X_train.rename(columns={'glucose_binned_numeric': 'avg_glucose_level'})


print(X_train)

"""now we train the model based on new training data and see the difference"""

import cupy as cp
from sklearn.metrics import classification_report
from collections import Counter
import time
import numpy as np
class KNNClassifier:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X_train, y_train):
        # Transfer data to GPU
        self.X_train = cp.asarray(X_train)
        self.y_train = cp.asarray(y_train)

    def predict(self, X_test):
        start_time = time.time()
        total_samples = len(X_test)
        predictions = []

        # Convert test data to GPU array
        X_test = cp.asarray(X_test)

        for i, x in enumerate(X_test):
            # Print progress
            elapsed_time = time.time() - start_time
            progress = (i + 1) / total_samples * 100
            estimated_total_time = elapsed_time / (i + 1) * total_samples
            remaining_time = estimated_total_time - elapsed_time

            print(f"Progress: {progress:.2f}% | Elapsed time: {elapsed_time:.2f}s | Estimated time left: {remaining_time:.2f}s", end="\r")

            predictions.append(self._predict(x))

        print()  # Newline after the progress bar
        return cp.asnumpy(cp.array(predictions))  # Convert back to CPU (NumPy) array

    def _predict(self, x):
        # Compute distances between x and all examples in the training set (using GPU)
        distances = cp.linalg.norm(self.X_train - x, axis=1)

        # Sort by distance and return indices of the first k neighbors
        k_indices = cp.argsort(distances)[:self.k]

        # Extract the labels of the nearest neighbors
        k_nearest_labels = self.y_train[k_indices]

        # Return the most common class label
        most_common = Counter(k_nearest_labels.get())  # Transfer back to CPU for Counter
        return most_common.most_common(1)[0][0]


# Convert DataFrames to NumPy arrays (if not already)
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train).flatten()

knn = KNNClassifier(k=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(classification_report(y_test, y_pred))

"""we can see results did'nt get better:)

we load the actual data again and just apply the standardization on age, bmi, and glucose level
"""

X_train = pd.DataFrame(X_train, columns=['id','gender','age','hypertension','heart_disease','ever_married','Residence_type','avg_glucose_level','bmi','work_type_0','work_type_1','work_type_2','work_type_3','work_type_4'])

# Standardization (manual) for 'age', 'avg_glucose_level', 'bmi'
for col in ['age', 'avg_glucose_level', 'bmi']:
    mean = X_train[col].mean()
    std = X_train[col].std()
    X_train[col ] = (X_train[col] - mean) / std

# Normalization (manual) for 'age', 'avg_glucose_level', 'bmi'
for col in ['age', 'avg_glucose_level', 'bmi']:
    min_val = X_train[col].min()
    max_val = X_train[col].max()
    X_train[col ] = (X_train[col] - min_val) / (max_val - min_val)

# Display the updated DataFrame
print(X_train)

X_train.T

X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train).flatten()

knn = KNNClassifier(k=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(classification_report(y_test, y_pred))

"""There is no change

we drop the gender because the correlation was low
"""

X_train = pd.DataFrame(X_train, columns=['id','gender','age','hypertension','heart_disease','ever_married','Residence_type','avg_glucose_level','bmi','work_type_0','work_type_1','work_type_2','work_type_3','work_type_4'])
X_train = X_train.drop(columns=['gender'])
print(X_train)

"""retraining the model"""

X_test = pd.DataFrame(X_test, columns=['id','gender','age','hypertension','heart_disease','ever_married','Residence_type','avg_glucose_level','bmi','work_type_0','work_type_1','work_type_2','work_type_3','work_type_4'])
X_test = X_test.drop(columns=['gender'])
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train).flatten()

knn = KNNClassifier(k=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(classification_report(y_test, y_pred))

"""it did not get better, load the data again, apply normalization and standardization again this time removing id"""

X_train = pd.DataFrame(X_train, columns=['id','gender','age','hypertension','heart_disease','ever_married','Residence_type','avg_glucose_level','bmi','work_type_0','work_type_1','work_type_2','work_type_3','work_type_4'])
X_train = X_train.drop(columns=['id'])
X_test = pd.DataFrame(X_test, columns=['id','gender','age','hypertension','heart_disease','ever_married','Residence_type','avg_glucose_level','bmi','work_type_0','work_type_1','work_type_2','work_type_3','work_type_4'])
X_test = X_test.drop(columns=['id'])
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train).flatten()

knn = KNNClassifier(k=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print(classification_report(y_test, y_pred))

"""it gets much better result:)

## part 6

here we should test impact of k values on our result, for this aim we considern training and getting results for a range of values
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

k_values = range(1, 21)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    print(f"Classification report for k={k}:\n")
    print(classification_report(y_test, y_pred))
    print("-" * 50)

# Plot the accuracy for each k value
plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, marker='o')
plt.title('Accuracy vs. k in KNN')
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.xticks(k_values)
plt.grid(True)
plt.show()

"""the best accuracy is for k=2

now changing distance caculation method in see results
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# Define the distance metrics to test
metrics = ['euclidean', 'manhattan', 'chebyshev', 'cosine']
accuracies = []

# Iterate over different distance metrics
for metric in metrics:
    # Create and train the KNN model
    knn = KNeighborsClassifier(n_neighbors=2, metric=metric)
    knn.fit(X_train, y_train)

    # Predict on the test data
    y_pred = knn.predict(X_test)

    # Calculate accuracy and store it
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

    # Print the classification report
    print(f"Classification report for metric={metric}:\n")
    print(classification_report(y_test, y_pred))
    print("-" * 50)

# Plot the accuracy for each distance metric
plt.figure(figsize=(10, 6))
plt.bar(metrics, accuracies, color='skyblue')
plt.title('Accuracy vs. Distance Metric in KNN')
plt.xlabel('Distance Metric')
plt.ylabel('Accuracy')
plt.grid(axis='y')
plt.show()

"""we get the best result with cosine calculation method and k =2"""